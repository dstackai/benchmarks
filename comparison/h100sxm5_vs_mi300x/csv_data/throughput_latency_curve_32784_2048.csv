gpu,input_len,output_len,batch_size,end_to_end_latency,tokens_per_second
8xH100-meta-Llama-31-405B-FP8,32784,2048,4,236.1237305,590.0635218
8xH100-meta-Llama-31-405B-FP8,32784,2048,8,457.6965823,608.8225492
8xH100-meta-Llama-31-405B-FP8,32784,2048,16,866.9613514,642.8337308
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,4,376.392776,370.1665092
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,8,477.3153248,583.7985615
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,16,673.9836962,826.8924058
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,32,1186.617763,939.3285983
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,64,2375.957974,938.2522858
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,4,129.3197857,1077.391207
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,8,158.0476773,1763.113542
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,16,223.162426,2497.33797
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,32,353.0339651,3157.271284
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,64,720.3670517,3094.600169
4xMi300x 1FP8-amd,32784,2048,4,158.1494814,880.9892944
4xMi300x 1FP8-amd,32784,2048,8,262.4562405,1061.723659
4xMi300x 1FP8-amd,32784,2048,16,523.8995579,1063.776427
4xMi300x 1FP8-amd,32784,2048,32,1048.253293,1063.315524
4xMi300x 1FP8-amd,32784,2048,64,2000.700276,1114.233864