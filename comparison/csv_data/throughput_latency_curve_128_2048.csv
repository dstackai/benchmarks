gpu,input_len,output_len,batch_size,end_to_end_latency,tokens_per_second
8xH100-meta-Llama-31-405B-FP8,128,2048,4,71.27394036,122.1203704
8xH100-meta-Llama-31-405B-FP8,128,2048,8,75.05596489,231.9335981
8xH100-meta-Llama-31-405B-FP8,128,2048,16,75.82455635,459.1652319
8xH100-meta-Llama-31-405B-FP8,128,2048,32,78.37005056,888.5026806
8xH100-meta-Llama-31-405B-FP8,128,2048,64,123.1509682,1130.839668
8xH100-meta-Llama-31-405B-FP8,128,2048,128,214.67,1297.443
8xH100-meta-Llama-31-405B-FP8,128,2048,256,399.64,1393.8654
8xMi300x-meta-Llama-31-405B-FP8,128,2048,4,279.115799,31.1841897
8xMi300x-meta-Llama-31-405B-FP8,128,2048,8,287.809995,60.4843484
8xMi300x-meta-Llama-31-405B-FP8,128,2048,16,290.992608,119.645651
8xMi300x-meta-Llama-31-405B-FP8,128,2048,32,293.69183,237.092056
8xMi300x-meta-Llama-31-405B-FP8,128,2048,64,307.312373,453.167565
8xMi300x-meta-Llama-31-405B-FP8,128,2048,128,340.863436,817.124897
8xMi300x-meta-Llama-31-405B-FP8,128,2048,256,438.203916,1271.22552
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,4,67.79069331,128.3952055
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,8,68.69831185,253.3977842
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,16,73.0668384,476.4952304
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,32,81.6235981,853.0866271
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,64,99.36810183,1401.496028
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,128,131.6356226,2115.901414
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,128,2048,256,227.2880827,2450.880809
4xMi300x 1FP8-amd,128,2048,4,94.17381532,92.42484198
4xMi300x 1FP8-amd,128,2048,8,97.83940387,177.9242239
4xMi300x 1FP8-amd,128,2048,16,101.5019775,343.0080957
4xMi300x 1FP8-amd,128,2048,32,113.7299072,612.2575997
4xMi300x 1FP8-amd,128,2048,64,136.9388266,1016.979651
4xMi300x 1FP8-amd,128,2048,128,195.5264158,1424.503174
4xMi300x 1FP8-amd,128,2048,256,431.6155709,1290.62999