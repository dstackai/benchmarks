gpu,input_len,output_len,batch_size,end_to_end_latency,tokens_per_second
8xH100-meta-Llama-31-405B-FP8,2048,2048,4,73.04,224.3991082
8xH100-meta-Llama-31-405B-FP8,2048,2048,8,78.6,416.0803779
8xH100-meta-Llama-31-405B-FP8,2048,2048,16,84.22,781.5082372
8xH100-meta-Llama-31-405B-FP8,2048,2048,32,144.09,911.1295814
8xH100-meta-Llama-31-405B-FP8,2048,2048,64,262.68,998.7796751
8xH100-meta-Llama-31-405B-FP8,2048,2048,128,474.87,1105.487042
8xH100-meta-Llama-31-405B-FP8,2048,2048,256,879.73,1196.34
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,4,266.06,58.34189395
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,8,274.47,111.8647335
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,16,284.33,218.6357561
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,32,299.45,418.4837956
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,64,341.75,741.1052995
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,128,419.36,1216.741493
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,256,611.82,1705.140233
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,4,68.37971305,239.6032283
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,8,70.53868689,464.5394101
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,16,78.69783408,832.7548117
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,32,96.35701944,1360.274537
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,64,125.3573432,2091.173866
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,128,186.8320667,2806.199221
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,256,334.4946058,3134.806905
4xMi300x 1FP8-amd,2048,2048,4,97.15577103,168.6364055
4xMi300x 1FP8-amd,2048,2048,8,103.1994496,317.5210734
4xMi300x 1FP8-amd,2048,2048,16,114.7069595,571.3341223
4xMi300x 1FP8-amd,2048,2048,32,138.4119717,946.9701092
4xMi300x 1FP8-amd,2048,2048,64,185.5635483,1412.691245
4xMi300x 1FP8-amd,2048,2048,128,442.6162474,1184.520458
4xMi300x 1FP8-amd,2048,2048,256,790.0569983,1327.215634