gpu,input_len,output_len,batch_size,end_to_end_latency,tokens_per_second
8xH100-meta-Llama-31-405B-FP8,2048,2048,4,73.04,224.3991082
8xH100-meta-Llama-31-405B-FP8,2048,2048,8,78.6,416.0803779
8xH100-meta-Llama-31-405B-FP8,2048,2048,16,84.22,781.5082372
8xH100-meta-Llama-31-405B-FP8,2048,2048,32,144.09,911.1295814
8xH100-meta-Llama-31-405B-FP8,2048,2048,64,262.68,998.7796751
8xH100-meta-Llama-31-405B-FP8,2048,2048,128,474.87,1105.487042
8xH100-meta-Llama-31-405B-FP8,2048,2048,256,879.73,1196.34
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,4,266.06,58.34189395
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,8,274.47,111.8647335
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,16,284.33,218.6357561
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,32,299.45,418.4837956
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,64,341.75,741.1052995
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,128,419.36,1216.741493
8xMi300x-meta-Llama-31-405B-FP8,2048,2048,256,611.82,1705.140233
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,4,59.2795396,276.385412
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,8,62.8160444,521.650166
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,16,70.8378626,925.154961
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,32,85.8132522,1527.40977
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,64,114.204847,2295.38419
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,128,171.64011,3054.57739
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV ,2048,2048,256,292.615437,3583.46098