gpu,input_len,output_len,batch_size,end_to_end_latency,tokens_per_second
8xH100-meta-Llama-31-405B-FP8,32784,2048,4,236.1237305,590.0635218
8xH100-meta-Llama-31-405B-FP8,32784,2048,8,457.6965823,608.8225492
8xH100-meta-Llama-31-405B-FP8,32784,2048,16,866.9613514,642.8337308
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,4,376.392776,370.1665092
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,8,477.3153248,583.7985615
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,16,673.9836962,826.8924058
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,32,1186.617763,939.3285983
8xMi300x-meta-Llama-31-405B-FP8,32784,2048,64,2375.957974,938.2522858
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,4,128.7406997,1082.237399
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,8,155.9300552,1787.057663
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,16,215.8930533,2581.426274
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,32,331.0764287,3366.666738
8xMi300x-amd-Llama-31-405B-Instruct-FP8-KV,32784,2048,64,587.0161377,3797.59236