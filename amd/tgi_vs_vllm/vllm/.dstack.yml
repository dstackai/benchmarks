type: dev-environment
name: dev-vllm-amd

image: runpod/pytorch:2.4.0-py3.10-rocm6.1.0-ubuntu22.04
env:
  - HUGGING_FACE_HUB_TOKEN
  - MODEL_ID=meta-llama/Llama-3.1-405B-Instruct

#init:
#  - export PATH=/opt/conda/envs/py_3.10/bin:$PATH
#  - wget https://github.com/ROCm/hipBLAS/archive/refs/tags/rocm-6.1.0.zip
#  - unzip rocm-6.1.0.zip
#  - cd hipBLAS-rocm-6.1.0
#  - python rmake.py
#  - cd ..
#  - git clone https://github.com/vllm-project/vllm.git
#  - cd vllm
#  - pip install triton
#  - pip uninstall torch -y
#  - pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1
#  - pip install /opt/rocm/share/amd_smi
#  - pip install --upgrade numba scipy huggingface-hub[cli]
#  - pip install "numpy<2"
#  - pip install -r requirements-rocm.txt
#  - wget -N https://github.com/ROCm/vllm/raw/fa78403/rocm_patch/libamdhip64.so.6 -P /opt/rocm/lib
#  - rm -f "$(python3 -c 'import torch; print(torch.__path__[0])')"/lib/libamdhip64.so*
#  - export PYTORCH_ROCM_ARCH="gfx90a;gfx942"
#  - pip install setuptools_scm
#  - python setup.py develop
#  - ROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 vllm serve $MODEL_ID --tensor-parallel-size=8 --disable-log-requests --disable-frontend-multiprocessing